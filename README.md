# Bias-Detection-SDXL-DPO-Text2Image-Model
Explores collective gender, race and ethnicity-specific bias in the Diffusion Model Alignment Using Direct Preference Optimization Text2Image model. 
![FTML Exam Paper Submission_page-0001](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/3fad1652-6f69-421a-9be2-16586f014aba)
![FTML Exam Paper Submission_page-0002](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/b545d4a7-0b1b-47be-9dd6-6d2647e849b9)
![FTML Exam Paper Submission_page-0003](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/819a19c5-9801-42e4-a4f7-8fc2948f7994)
![FTML Exam Paper Submission_page-0004](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/7c836e9d-bc22-4834-8940-b695b5b0fd8b)
![FTML Exam Paper Submission_page-0005](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/c8273576-66d4-4a4a-8499-02dcbe0f6d1f)
![FTML Exam Paper Submission_page-0006](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/77b27337-6f5f-43a1-9577-d72bca6de878)
![FTML Exam Paper Submission_page-0007](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/8fc5f5d2-2865-4a52-a362-190d4c236d92)
![FTML Exam Paper Submission_page-0008](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/ce0b6a9f-ef89-4ea8-98c0-2c96bbab0489)
![FTML Exam Paper Submission_page-0009](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/46316100-82fc-4580-9224-8cc817fa5629)
![FTML Exam Paper Submission_page-0010](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/dcb83c6b-ffb8-4622-b3e0-cd27269df26d)
![FTML Exam Paper Submission_page-0011](https://github.com/sm-ak-r33/Bias-Detection-SDXL-DPO-Text2Image-Model/assets/116037698/c7fb4eda-db23-456e-8034-a835a0fbe341)
